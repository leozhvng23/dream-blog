---
layout: post
title: Week 10 - In the Trenches of Development
---
![Figma Prototype V2](https://leozhvng23.github.io/dream-blog/images/week10.JPG)
The tenth week felt like a whirlwind. Koushik and I dived deep into development, fueled by the ambition to bring our system to life. Integrating backend and frontend systems can often feel like fitting puzzle pieces together, especially when you're striving for real-time performance. We chose to use MQTT, a lightweight messaging protocol, which added another layer of intricacy to our development journey.

Our initial plan was simple. Koushik would publish data to the MQTT server, which I would then fetch in the frontend. While it sounded simple on paper, reality proved otherwise. Each hurdle, no matter how small, required meticulous troubleshooting. Still, every issue we overcame brought an indescribable feeling of accomplishment.

The week's spotlight, however, was on refining the veering feedback mechanism. Our initial model relied on predicted user headings obtained from a Kalman filter. But predictions, data transfers, and processing times combined created noticeable lags, detracting from the user experience. 

It was time for a change. 

Inspired by the iPhone's built-in compass, we embarked on a field test. Gaurav, Koushik, and I, with our iPhones, simulated pedestrian scenarios on the streets. The results were promising. The compass's real-time accuracy hinted at its potential use in our application.

Back in the lab, we began a much needed whiteboard brainstorming session. Sketches, angles, calculations - we mapped out the math and logic for the compass-based approach. My task then shifted to the world of Swift documentation. Luckily, accessing the iPhone compass readings turned out to be a breeze. A simple 30-degree offset adjustment transformed the raw compass data into actionable headings, based on our map's orientation.

The improved veering mechanism felt intuitive. By calculating the angle difference between the user's current direction and the target direction, we could gauge veering in real-time. And to elevate the user experience further, haptic and audio feedback was incorporated. The phone's vibrations now resonate with the user's alignment, growing stronger as they get closer to the right direction.

With our navigation mechanism refined, next week will usher in a fresh challenge. We're stepping outside, testing our system in the wild. It's one thing for our app to function flawlessly in a controlled environment, but the real test lies out there on the streets. Moreover, gearing up for our tests with BLV users remains a priority. Onward!
